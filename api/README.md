# ğŸ SleepSafe Backend API

FastAPI backend for drowsiness detection telemetry and MLOps.

## ğŸ—ï¸ Architecture

```
api/
â”œâ”€â”€ main.py              # FastAPI application entry point
â”œâ”€â”€ models/              # Pydantic models for validation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ models.py        # Request/response models
â”œâ”€â”€ services/            # Business logic layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ services.py      # MLflow, Detection, Analytics services
â”œâ”€â”€ mlops/               # Machine learning operations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ train_model.py   # Model training script
â”œâ”€â”€ pyproject.toml       # UV dependencies
â”œâ”€â”€ Dockerfile           # Container configuration
â””â”€â”€ .gitignore           # Git exclusions
```

## ğŸš€ Quick Start

### Using UV (Recommended)

```bash
cd api

# Install dependencies
uv sync

# Run development server
uv run uvicorn main:app --reload

# Run ML training
uv run python mlops/train_model.py
```

### API Access

- **API**: http://localhost:8000
- **Docs**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ“¡ API Endpoints

### Health & Status

- `GET /` - API root information
- `GET /health` - Health check with uptime

### Telemetry

- `POST /telemetry` - Log single detection event
- `POST /telemetry/batch` - Log multiple events

**Example Request:**
```json
{
  "ear_value": 0.15,
  "is_drowsy": true,
  "duration_ms": 3500
}
```

### Analytics

- `GET /statistics` - Overall detection statistics
- `GET /dashboard` - Comprehensive dashboard data
- `GET /events/recent?limit=50` - Recent events

### MLOps

- `POST /metrics/model` - Log model performance metrics
- `GET /mlflow/runs?max_results=10` - Get MLflow runs

### Admin

- `DELETE /events/cache` - Clear event cache

## ğŸ§ª Testing

```bash
# Test health endpoint
curl http://localhost:8000/health

# Log detection event
curl -X POST http://localhost:8000/telemetry \
  -H "Content-Type: application/json" \
  -d '{"ear_value": 0.2, "is_drowsy": true, "duration_ms": 2000}'

# Get statistics
curl http://localhost:8000/statistics
```

## ğŸ³ Docker

### Build & Run

```bash
# From project root
docker compose up backend -d

# View logs
docker compose logs -f backend

# Run ML training
docker compose run --rm ml_training
```

## ğŸ“Š MLflow Integration

The backend integrates MLflow for experiment tracking:

- **Tracking URI**: `sqlite:///mlflow.db`
- **Experiment**: `drowsiness_detection`
- **Metrics**: EAR values, drowsiness events, model performance

### View MLflow UI

```bash
# Start MLflow UI
docker compose up mlflow_ui

# Access at http://localhost:5001
```

## ğŸ”§ Configuration

### Environment Variables

- `MLFLOW_TRACKING_URI` - MLflow tracking database (default: `sqlite:///mlflow.db`)

### Dependencies

- **fastapi** - Web framework
- **uvicorn** - ASGI server
- **mlflow** - Experiment tracking
- **pydantic** - Data validation
- **scikit-learn** - ML training
- **numpy** - Numerical computing

## ğŸ“ Development

### Project Structure

- `models/` - Data validation schemas
- `services/` - Business logic (MLflow, Detection, Analytics)
- `mlops/` - ML training scripts

### Adding New Endpoints

1. Define Pydantic models in `models/models.py`
2. Add business logic to appropriate service in `services/`
3. Create endpoint in `main.py`
4. Update this README

## ğŸ§© Service Layer

### MLflowService
- Log detection events
- Log model metrics
- Query experiment runs

### DetectionService
- Process detection events
- Calculate severity
- Maintain event cache
- Provide statistics

### AnalyticsService
- Hourly summaries
- Dashboard data
- System status

## ğŸš¨ Error Handling

All endpoints return standardized error responses:

```json
{
  "detail": "Error message",
  "status_code": 500,
  "timestamp": "2025-12-27T12:00:00.000Z"
}
```

## ğŸ“¦ Deployment

### Production Setup

1. **Environment**: Set `MLFLOW_TRACKING_URI` to production database
2. **CORS**: Configure allowed origins in `main.py`
3. **Database**: Use PostgreSQL for MLflow instead of SQLite
4. **Monitoring**: Add application monitoring (Sentry, Prometheus)

### Docker Compose

```bash
# Start all services
docker compose up -d

# Frontend: http://localhost:80
# Backend: http://localhost:8000
# MLflow: http://localhost:5001
```

## ğŸ”¬ Machine Learning

### Training Pipeline

The `mlops/train_model.py` script:
1. Generates synthetic drowsiness data
2. Trains Random Forest classifier
3. Logs metrics to MLflow
4. Registers model

**Run Training:**
```bash
uv run python mlops/train_model.py
```

## ğŸ“ˆ Monitoring

- **Logs**: Check `/api/logs` or `docker compose logs backend`
- **Health**: Monitor `/health` endpoint
- **MLflow**: Track experiments at http://localhost:5001

## ğŸ¤ Contributing

1. Add features to appropriate service
2. Update Pydantic models
3. Add tests (future)
4. Update documentation

---

**Status**: âœ… Backend Running  
**Port**: 8000  
**Docs**: http://localhost:8000/docs
